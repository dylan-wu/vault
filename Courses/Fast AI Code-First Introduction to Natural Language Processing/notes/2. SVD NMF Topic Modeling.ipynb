{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "Topic modeling is a fun way to start our study of NLP. We will use two popular **matrix decomposition techniques**.\n",
    "\n",
    "We start with a **term-document matrix**:\n",
    "\n",
    "![[Pasted image 20220607090648.png | 750]]\n",
    "\n",
    "We can decompose this into one tall thin matrix times one wide short matrix (possibly with a diagnoal matrix in between).\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Consider the most extreme case\n",
    "\n",
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\PersonalVault-2\\Courses\\Fast AI Code-First Introduction to Natural Language Processing\\notes\\2. SVD NMF Topic Modeling.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/PersonalVault-2/Courses/Fast%20AI%20Code-First%20Introduction%20to%20Natural%20Language%20Processing/notes/2.%20SVD%20NMF%20Topic%20Modeling.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/PersonalVault-2/Courses/Fast%20AI%20Code-First%20Introduction%20to%20Natural%20Language%20Processing/notes/2.%20SVD%20NMF%20Topic%20Modeling.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m fetch_20newsgroups\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/PersonalVault-2/Courses/Fast%20AI%20Code-First%20Introduction%20to%20Natural%20Language%20Processing/notes/2.%20SVD%20NMF%20Topic%20Modeling.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m \u001b[39mimport\u001b[39;00m decomposition\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import decomposition\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)\n",
    "newsgroups_train.filenames.shape, newsgroups_train.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(newsgroups_train.data[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics, num_top_words = 6, 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words, stemming, lemmatization\n",
    "\n",
    "### Stop words\n",
    "\n",
    "From Intro to Information Retrieval:\n",
    "\n",
    "Some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words.\n",
    "\n",
    "The general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists.\n",
    "\n",
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "sorted(list(stop_words.ENGLISH_STOP_WORDS))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and lemmatization\n",
    "\n",
    "from Information Retrieval textbook:\n",
    "\n",
    "Are the below words the same?\n",
    "\n",
    "organize, organizes, and organizing\n",
    "\n",
    "democracy, democratic, and democratization\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the words.\n",
    "\n",
    "Lemmatization uses the rules about a language. The resulting tokens are all actual words\n",
    "\n",
    "\"Stemming is the poor-manâ€™s lemmatization.\" (Noah Smith, 2011) Stemming is a crude heuristic that chops the ends off of words. The resulting tokens may not be actual words. Stemming is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = stem.WordNetLemmatizer()\n",
    "porter = stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['feet', 'foot', 'foots', 'footing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[wnl.lemmatize(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[porter.stem(word) for word in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn! Now, try lemmatizing and stemming the following collections of words:\n",
    "\n",
    "- fly, flies, flying\n",
    "- organize, organizes, organizing\n",
    "- universe, university\n",
    "\n",
    "fastai/course-nlp\n",
    "\n",
    "Stemming and lemmatization are language dependent. Languages with more complex morphologies may show bigger benefits. For example, Sanskrit has a very large number of verb forms.\n",
    "\n",
    "### Spacy\n",
    "\n",
    "Stemming and lemmatization are implementation dependent.\n",
    "\n",
    "Spacy is a very modern & fast nlp library. Spacy is opinionated, in that it typically offers one highly optimized way to do something (whereas nltk offers a huge variety of ways, although they are usually not as optimized).\n",
    "\n",
    "You will need to install it.\n",
    "\n",
    "if you use conda:\n",
    "\n",
    "conda install -c conda-forge spacy\n",
    "if you use pip:\n",
    "\n",
    "pip install -U spacy\n",
    "You will then need to download the English model:\n",
    "\n",
    "spacy -m download en_core_web_sm"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
